{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVU73e6CyBhD",
    "outputId": "54f04ebb-4a2b-472e-9f5d-030bef488c4c"
   },
   "outputs": [],
   "source": [
    "# !pip install PySastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cuiLFZ5PS4Sn"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "Nl7b8_C3vdG8",
    "outputId": "9093f2b0-2808-484c-e9f9-2d9c66484d8d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>TTEXT</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997_504851.txt</td>\n",
       "      <td>Nah, sekarang saya baru saja bangun dari tidur...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997_605191.txt</td>\n",
       "      <td>Nah, di sini kita pergi dengan arus kesadaran ...</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997_687252.txt</td>\n",
       "      <td>Keyboard terbuka dan tombol untuk mendorong. H...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997_568848.txt</td>\n",
       "      <td>Aku tidak percaya itu! Ini benar-benar terjadi...</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997_688160.txt</td>\n",
       "      <td>Nah, di sini aku pergi dengan aliran tua yang ...</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           #AUTHID                                              TTEXT cEXT  \\\n",
       "0  1997_504851.txt  Nah, sekarang saya baru saja bangun dari tidur...    n   \n",
       "1  1997_605191.txt  Nah, di sini kita pergi dengan arus kesadaran ...    n   \n",
       "2  1997_687252.txt  Keyboard terbuka dan tombol untuk mendorong. H...    n   \n",
       "3  1997_568848.txt  Aku tidak percaya itu! Ini benar-benar terjadi...    y   \n",
       "4  1997_688160.txt  Nah, di sini aku pergi dengan aliran tua yang ...    y   \n",
       "\n",
       "  cNEU cAGR cCON cOPN  \n",
       "0    y    y    n    y  \n",
       "1    n    y    n    n  \n",
       "2    y    n    y    y  \n",
       "3    n    y    y    n  \n",
       "4    n    y    n    y  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATASET SOURCE FROM https://github.com/dbrehmer/Knowself/blob/master/data/mypersonality/essays.csv\"\n",
    "#DATASET USED IN THIS NOTEBOOK IS DATASET FROM THE SOURCE THAT HAS BEEN TRANSLATED TO BAHASA INDONESIA USING GOOGLE TRANSLATE\n",
    "\n",
    "DATASET_URL = \"https://raw.githubusercontent.com/lazuardi100/Hexa-Engineer/ML/Dataset/dataset.csv\"\n",
    "df = pd.read_csv(DATASET_URL, sep =',')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KSa84m-fWk-m",
    "outputId": "f49cc4ed-1250-4969-eaa7-29046ca77d09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       0\n",
       "2       1\n",
       "3       0\n",
       "4       1\n",
       "       ..\n",
       "2462    0\n",
       "2463    1\n",
       "2464    0\n",
       "2465    1\n",
       "2466    1\n",
       "Name: cOPN, Length: 2467, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def changeLabel(labels):\n",
    "  for index, values in enumerate(labels.values):\n",
    "    if values == 'n':\n",
    "      labels[index] = 0\n",
    "    else:\n",
    "      labels[index] = 1\n",
    "  return labels\n",
    "\n",
    "changeLabel(df['cEXT'])\n",
    "changeLabel(df['cNEU'])\n",
    "changeLabel(df['cAGR'])\n",
    "changeLabel(df['cCON'])\n",
    "changeLabel(df['cOPN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "joE3yHq3uJ2U"
   },
   "outputs": [],
   "source": [
    "#Text Preprocessing referenced from https://github.com/ksnugroho/basic-text-preprocessing/blob/master/text-preprocessing.ipynb\n",
    "import string\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "def preprocess_text(text):\n",
    "  #lowercase all character in the text\n",
    "  text = text.lower()\n",
    "  #remove stripes\n",
    "  words = text.split(' ')\n",
    "  text = ''\n",
    "  for word in words:\n",
    "    if '-' in word:\n",
    "      word = word.split('-')[0]\n",
    "    text = text + word + ' '\n",
    "  #remove punctuation\n",
    "  text = text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "  #remove leading and trailing whitespace\n",
    "  text = text.strip()\n",
    "  #remove StopWord\n",
    "  stopword = StopWordRemoverFactory().create_stop_word_remover()\n",
    "  text = stopword.remove(text)\n",
    "  #stemming\n",
    "  stemmer = StemmerFactory().create_stemmer()\n",
    "  text = stemmer.stem(text)\n",
    "  return text\n",
    "\n",
    "df['TTEXT'] = df['TTEXT'].map(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1ETSRWdeh74v"
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "train_dataset, val_dataset, test_dataset = np.split(df, [int(.95 * len(df)), int(.975 * len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XpCdmoNPasJD"
   },
   "outputs": [],
   "source": [
    "train_data = train_dataset['TTEXT']\n",
    "# train_label_ext  = train_dataset['cEXT']\n",
    "# train_label_neu  = train_dataset['cNEU']\n",
    "train_label_agr  = train_dataset['cAGR']\n",
    "train_label_con  = train_dataset['cCON']\n",
    "train_label_opn  = train_dataset['cOPN']\n",
    "\n",
    "val_data = val_dataset['TTEXT']\n",
    "# val_label_ext  = val_dataset['cEXT']\n",
    "# val_label_neu  = val_dataset['cNEU']\n",
    "val_label_agr  = val_dataset['cAGR']\n",
    "val_label_con  = val_dataset['cCON']\n",
    "val_label_opn  = val_dataset['cOPN']\n",
    "\n",
    "test_data = test_dataset['TTEXT']\n",
    "# test_label_ext  = test_dataset['cEXT']\n",
    "# test_label_neu  = test_dataset['cNEU']\n",
    "test_label_agr  = test_dataset['cAGR']\n",
    "test_label_con  = test_dataset['cCON']\n",
    "test_label_opn  = test_dataset['cOPN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8FmqfVpRvbVo"
   },
   "outputs": [],
   "source": [
    "#Create Data Pipeline\n",
    "\n",
    "# train_dataset_ext = tf.data.Dataset.from_tensor_slices((train_data.values, train_label_ext.values.astype(dtype=np.float32)))\n",
    "# train_dataset_neu = tf.data.Dataset.from_tensor_slices((train_data.values, train_label_neu.values.astype(dtype=np.float32)))\n",
    "train_dataset_agr = tf.data.Dataset.from_tensor_slices((train_data.values, train_label_agr.values.astype(dtype=np.float32)))\n",
    "train_dataset_con = tf.data.Dataset.from_tensor_slices((train_data.values, train_label_con.values.astype(dtype=np.float32)))\n",
    "train_dataset_opn = tf.data.Dataset.from_tensor_slices((train_data.values, train_label_opn.values.astype(dtype=np.float32)))\n",
    "# \n",
    "# val_dataset_ext = tf.data.Dataset.from_tensor_slices((val_data.values, val_label_ext.values.astype(dtype=np.float32)))\n",
    "# val_dataset_neu = tf.data.Dataset.from_tensor_slices((val_data.values, val_label_neu.values.astype(dtype=np.float32)))\n",
    "val_dataset_agr = tf.data.Dataset.from_tensor_slices((val_data.values, val_label_agr.values.astype(dtype=np.float32)))\n",
    "val_dataset_con = tf.data.Dataset.from_tensor_slices((val_data.values, val_label_con.values.astype(dtype=np.float32)))\n",
    "val_dataset_opn = tf.data.Dataset.from_tensor_slices((val_data.values, val_label_opn.values.astype(dtype=np.float32)))\n",
    "# \n",
    "# test_dataset_ext = tf.data.Dataset.from_tensor_slices((test_data.values, test_label_ext.values.astype(dtype=np.float32)))\n",
    "# test_dataset_neu = tf.data.Dataset.from_tensor_slices((test_data.values, test_label_neu.values.astype(dtype=np.float32)))\n",
    "test_dataset_agr = tf.data.Dataset.from_tensor_slices((test_data.values, test_label_agr.values.astype(dtype=np.float32)))\n",
    "test_dataset_con = tf.data.Dataset.from_tensor_slices((test_data.values, test_label_con.values.astype(dtype=np.float32)))\n",
    "test_dataset_opn = tf.data.Dataset.from_tensor_slices((test_data.values, test_label_opn.values.astype(dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_examples = tf.data.experimental.cardinality(train_dataset_agr).numpy()\n",
    "\n",
    "# ext_train_batches = train_dataset_ext.shuffle(num_examples // 4).batch(batch_size).prefetch(1)\n",
    "# ext_val_batches = val_dataset_ext.batch(batch_size).prefetch(1)\n",
    "# ext_test_batches = test_dataset_ext.batch(batch_size)\n",
    "\n",
    "# neu_train_batches = train_dataset_neu.shuffle(num_examples // 4).batch(batch_size).prefetch(1)\n",
    "# neu_val_batches = val_dataset_neu.batch(batch_size).prefetch(1)\n",
    "# neu_test_batches = test_dataset_neu.batch(batch_size)\n",
    "\n",
    "agr_train_batches = train_dataset_agr.shuffle(num_examples // 4).batch(batch_size).prefetch(1)\n",
    "agr_val_batches = val_dataset_agr.batch(batch_size).prefetch(1)\n",
    "agr_test_batches = test_dataset_agr.batch(batch_size)\n",
    "\n",
    "con_train_batches = train_dataset_con.shuffle(num_examples // 4).batch(batch_size).prefetch(1)\n",
    "con_val_batches = val_dataset_con.batch(batch_size).prefetch(1)\n",
    "con_test_batches = test_dataset_con.batch(batch_size)\n",
    "\n",
    "opn_train_batches = train_dataset_opn.shuffle(num_examples // 4).batch(batch_size).prefetch(1)\n",
    "opn_val_batches = val_dataset_opn.batch(batch_size).prefetch(1)\n",
    "opn_test_batches = test_dataset_opn.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "z6Jp5BGiJnR9"
   },
   "outputs": [],
   "source": [
    "#Some model are commented to save the memory\n",
    "embedding = \"https://tfhub.dev/google/nnlm-id-dim128/2\"\n",
    "\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)\n",
    "\n",
    "# model_ext = tf.keras.Sequential([\n",
    "#           hub_layer,\n",
    "#           tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, 1)),\n",
    "#           tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16,kernel_regularizer=tf.keras.regularizers.L2(1e-5),recurrent_regularizer=tf.keras.regularizers.L2(1e-6))),\n",
    "#           tf.keras.layers.Dense(8, activation='relu'),\n",
    "#           tf.keras.layers.Dropout(0.4),\n",
    "#           tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "# model_neu = tf.keras.Sequential([\n",
    "#           hub_layer,\n",
    "#           tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, 1)),\n",
    "#           tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16,kernel_regularizer=tf.keras.regularizers.L2(1e-5),recurrent_regularizer=tf.keras.regularizers.L2(1e-6))),\n",
    "#           tf.keras.layers.Dense(8, activation='relu'),\n",
    "#           tf.keras.layers.Dropout(0.4),\n",
    "#           tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "model_agr = tf.keras.Sequential([\n",
    "          hub_layer,\n",
    "          tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, 1)),\n",
    "          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16,kernel_regularizer=tf.keras.regularizers.L2(1e-5),recurrent_regularizer=tf.keras.regularizers.L2(1e-6))),\n",
    "          tf.keras.layers.Dense(8, activation='relu'),\n",
    "          tf.keras.layers.Dropout(0.4),\n",
    "          tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "model_con = tf.keras.Sequential([\n",
    "          hub_layer,\n",
    "          tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, 1)),\n",
    "          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16,kernel_regularizer=tf.keras.regularizers.L2(1e-5),recurrent_regularizer=tf.keras.regularizers.L2(1e-6))),\n",
    "          tf.keras.layers.Dense(8, activation='relu'),\n",
    "          tf.keras.layers.Dropout(0.4),\n",
    "          tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "model_opn = tf.keras.Sequential([\n",
    "          hub_layer,\n",
    "          tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, 1)),\n",
    "          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16,kernel_regularizer=tf.keras.regularizers.L2(1e-5),recurrent_regularizer=tf.keras.regularizers.L2(1e-6))),\n",
    "          tf.keras.layers.Dense(8, activation='relu'),\n",
    "          tf.keras.layers.Dropout(0.4),\n",
    "          tf.keras.layers.Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early Stopping\n",
    "class highAccCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('accuracy')>0.90):\n",
    "      print(\"\\nReached 90% accuracy so cancelling training to prevent overfitting the model!\")\n",
    "      self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B4G3sVX9Tzur",
    "outputId": "75f08dcb-443f-44a3-d8e9-d7d6ee6e2018"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "19/19 [==============================] - 17s 716ms/step - loss: 0.6952 - accuracy: 0.5023 - val_loss: 0.6982 - val_accuracy: 0.4032\n",
      "Epoch 2/10\n",
      "19/19 [==============================] - 13s 660ms/step - loss: 0.6916 - accuracy: 0.5190 - val_loss: 0.6950 - val_accuracy: 0.4032\n",
      "Epoch 3/10\n",
      "19/19 [==============================] - 13s 682ms/step - loss: 0.6867 - accuracy: 0.5190 - val_loss: 0.6918 - val_accuracy: 0.4032\n",
      "Epoch 4/10\n",
      "19/19 [==============================] - 13s 670ms/step - loss: 0.6800 - accuracy: 0.5198 - val_loss: 0.6863 - val_accuracy: 0.4677\n",
      "Epoch 5/10\n",
      "19/19 [==============================] - 13s 656ms/step - loss: 0.6554 - accuracy: 0.6018 - val_loss: 0.6466 - val_accuracy: 0.6774\n",
      "Epoch 6/10\n",
      "19/19 [==============================] - 12s 629ms/step - loss: 0.6160 - accuracy: 0.6641 - val_loss: 0.6380 - val_accuracy: 0.6290\n",
      "Epoch 7/10\n",
      "19/19 [==============================] - 11s 603ms/step - loss: 0.5554 - accuracy: 0.7260 - val_loss: 0.6194 - val_accuracy: 0.6452\n",
      "Epoch 8/10\n",
      "19/19 [==============================] - 12s 603ms/step - loss: 0.4738 - accuracy: 0.7930 - val_loss: 0.6323 - val_accuracy: 0.6290\n",
      "Epoch 9/10\n",
      "19/19 [==============================] - 11s 597ms/step - loss: 0.4019 - accuracy: 0.8468 - val_loss: 0.6956 - val_accuracy: 0.6129\n",
      "Epoch 10/10\n",
      "19/19 [==============================] - 11s 593ms/step - loss: 0.3114 - accuracy: 0.9027 - val_loss: 0.7559 - val_accuracy: 0.6452\n",
      "\n",
      "Reached 90% accuracy so cancelling training to prevent overfitting the model!\n"
     ]
    }
   ],
   "source": [
    "# model_ext.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# history_ext = model_ext.fit(ext_train_batches,\n",
    "#                         validation_data=ext_val_batches,\n",
    "#                         epochs=10,\n",
    "#                         callbacks = [highAccCallback()]\n",
    "#                         )\n",
    "\n",
    "# model_neu.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# history_neu = model_neu.fit(neu_train_batches,\n",
    "#                         validation_data=neu_val_batches,\n",
    "#                         epochs=10,\n",
    "#                         callbacks = [highAccCallback()]\n",
    "#                         )\n",
    "\n",
    "# model_agr.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# history_agr = model_agr.fit(agr_train_batches,\n",
    "#                         validation_data=agr_val_batches,\n",
    "#                         epochs=10,\n",
    "#                         callbacks = [highAccCallback()]\n",
    "#                         )\n",
    "\n",
    "# model_con.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# history_con = model_con.fit(con_train_batches,\n",
    "#                         validation_data=con_val_batches,\n",
    "#                         epochs=10,\n",
    "#                         callbacks = [highAccCallback()]\n",
    "#                         )\n",
    "\n",
    "model_opn.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history_opn = model_opn.fit(opn_train_batches,\n",
    "                        validation_data=opn_val_batches,\n",
    "                        epochs=10,\n",
    "                        callbacks = [highAccCallback()]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tmMljAepY9O",
    "outputId": "517a5277-d738-421f-a54d-c19c5a6fce95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1429 - accuracy: 0.4194\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.3548 - accuracy: 0.4194\n",
      "1/1 [==============================] - 9s 9s/step - loss: 0.7619 - accuracy: 0.5968\n"
     ]
    }
   ],
   "source": [
    "# result_ext = model_ext.evaluate(ext_test_batches)\n",
    "# result_neu = model_neu.evaluate(neu_test_batches)\n",
    "result_agr = model_agr.evaluate(agr_test_batches)\n",
    "result_con = model_con.evaluate(con_test_batches)\n",
    "result_opn = model_opn.evaluate(opn_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "id": "hao-CixcpzEm",
    "outputId": "98b6704b-5c6a-4c1b-cfe3-d942bd446ccc"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "qJbsLqSYOZJO"
   },
   "outputs": [],
   "source": [
    "# model_ext.save('ext_model.h5')\n",
    "# model_neu.save('neu_model.h5')\n",
    "model_agr.save('agr_model.h5')\n",
    "model_con.save('con_model.h5')\n",
    "model_opn.save('opn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uI1Xw-Ael8iy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://agr_model_v2.h5 [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "/ [1 files][  1.3 GiB/  1.3 GiB]                                                \n",
      "Operation completed over 1 objects/1.3 GiB.                                      \n",
      "Copying file://con_model_v2.h5 [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "- [1 files][  1.3 GiB/  1.3 GiB]                                                \n",
      "Operation completed over 1 objects/1.3 GiB.                                      \n",
      "Copying file://ext_model_v2.h5 [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "| [1 files][  1.3 GiB/  1.3 GiB]                                                \n",
      "Operation completed over 1 objects/1.3 GiB.                                      \n",
      "Copying file://neu_model_v2.h5 [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "/ [1 files][  1.3 GiB/  1.3 GiB]                                                \n",
      "Operation completed over 1 objects/1.3 GiB.                                      \n",
      "Copying file://opn_model_v2.h5 [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "\\ [0 files][769.6 MiB/  1.3 GiB]    2.4 MiB/s                                   \r"
     ]
    }
   ],
   "source": [
    "!gsutil cp agr_model_v2.h5 gs://b21-cap0116\n",
    "!gsutil cp con_model_v2.h5 gs://b21-cap0116\n",
    "!gsutil cp ext_model_v2.h5 gs://b21-cap0116\n",
    "!gsutil cp neu_model_v2.h5 gs://b21-cap0116\n",
    "!gsutil cp opn_model_v2.h5 gs://b21-cap0116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPs7PaL8NadN6NKyP0B+cqq",
   "name": "Implementation.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-5.m70",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m70"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
