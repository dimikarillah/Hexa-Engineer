{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implementation.ipynb",
      "provenance": []
    },
    "environment": {
      "name": "tf2-gpu.2-5.m70",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m70"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuiLFZ5PS4Sn"
      },
      "source": [
        "#Import library required for project\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl7b8_C3vdG8"
      },
      "source": [
        "#DATASET SOURCE FROM https://github.com/dbrehmer/Knowself/blob/master/data/mypersonality/essays.csv\"\n",
        "#DATASET USED IN THIS NOTEBOOK IS DATASET FROM THE SOURCE THAT HAS BEEN TRANSLATED TO BAHASA INDONESIA USING GOOGLE TRANSLATE\n",
        "\n",
        "DATASET_URL = \"https://raw.githubusercontent.com/lazuardi100/Hexa-Engineer/ML/Dataset/dataset.csv\"\n",
        "df = pd.read_csv(DATASET_URL, sep =',')\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSa84m-fWk-m"
      },
      "source": [
        "#Method to change label from y/n into 1/0\n",
        "def changeLabel(labels):\n",
        "  for index, values in enumerate(labels.values):\n",
        "    if values == 'n':\n",
        "      labels[index] = 0\n",
        "    else:\n",
        "      labels[index] = 1\n",
        "  return labels\n",
        "\n",
        "changeLabel(df['cEXT'])\n",
        "changeLabel(df['cNEU'])\n",
        "changeLabel(df['cAGR'])\n",
        "changeLabel(df['cCON'])\n",
        "changeLabel(df['cOPN'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joE3yHq3uJ2U"
      },
      "source": [
        "#Text Preprocessing referenced from https://github.com/ksnugroho/basic-text-preprocessing/blob/master/text-preprocessing.ipynb\n",
        "def preprocess_text(text):\n",
        "  #lowercase all character in the text\n",
        "  text = text.lower()\n",
        "  #remove punctuation\n",
        "  text = text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "  #remove leading and trailing whitespace\n",
        "  text = text.strip()\n",
        "  #remove StopWord\n",
        "  stopword = StopWordRemoverFactory().create_stop_word_remover()\n",
        "  text = stopword.remove(text)\n",
        "  #stemming\n",
        "  stemmer = StemmerFactory().create_stemmer()\n",
        "  text = stemmer.stem(text)\n",
        "  return text\n",
        "\n",
        "df['TTEXT'] = df['TTEXT'].map(preprocess_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ETSRWdeh74v"
      },
      "source": [
        "#Shuffle the dataset and split it for train, validation, and test\n",
        "\n",
        "df = df.sample(frac=1)\n",
        "train_dataset, val_dataset, test_dataset = np.split(df, [int(.95 * len(df)), int(.975 * len(df))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpCdmoNPasJD"
      },
      "source": [
        "#Separate the data and the label from each other\n",
        "\n",
        "train_data = train_dataset['TTEXT']\n",
        "train_label_ext  = train_dataset['cEXT']\n",
        "train_label_neu  = train_dataset['cNEU']\n",
        "train_label_agr  = train_dataset['cAGR']\n",
        "train_label_con  = train_dataset['cCON']\n",
        "train_label_opn  = train_dataset['cOPN']\n",
        "\n",
        "val_data = val_dataset['TTEXT']\n",
        "val_label_ext  = val_dataset['cEXT']\n",
        "val_label_neu  = val_dataset['cNEU']\n",
        "val_label_agr  = val_dataset['cAGR']\n",
        "val_label_con  = val_dataset['cCON']\n",
        "val_label_opn  = val_dataset['cOPN']\n",
        "\n",
        "test_data = test_dataset['TTEXT']\n",
        "test_label_ext  = test_dataset['cEXT']\n",
        "test_label_neu  = test_dataset['cNEU']\n",
        "test_label_agr  = test_dataset['cAGR']\n",
        "test_label_con  = test_dataset['cCON']\n",
        "test_label_opn  = test_dataset['cOPN']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FmqfVpRvbVo"
      },
      "source": [
        "#Create Data Pipeline\n",
        "\n",
        "train_dataset_ext = tf.data.Dataset.from_tensor_slices((train_data.values, train_label_ext.values.astype(dtype=np.float32)))\n",
        "train_dataset_neu = tf.data.Dataset.from_tensor_slices((train_data.values, train_label_neu.values.astype(dtype=np.float32)))\n",
        "train_dataset_agr = tf.data.Dataset.from_tensor_slices((train_data.values, train_label_agr.values.astype(dtype=np.float32)))\n",
        "train_dataset_con = tf.data.Dataset.from_tensor_slices((train_data.values, train_label_con.values.astype(dtype=np.float32)))\n",
        "train_dataset_opn = tf.data.Dataset.from_tensor_slices((train_data.values, train_label_opn.values.astype(dtype=np.float32)))\n",
        "\n",
        "val_dataset_ext = tf.data.Dataset.from_tensor_slices((val_data.values, val_label_ext.values.astype(dtype=np.float32)))\n",
        "val_dataset_neu = tf.data.Dataset.from_tensor_slices((val_data.values, val_label_neu.values.astype(dtype=np.float32)))\n",
        "val_dataset_agr = tf.data.Dataset.from_tensor_slices((val_data.values, val_label_agr.values.astype(dtype=np.float32)))\n",
        "val_dataset_con = tf.data.Dataset.from_tensor_slices((val_data.values, val_label_con.values.astype(dtype=np.float32)))\n",
        "val_dataset_opn = tf.data.Dataset.from_tensor_slices((val_data.values, val_label_opn.values.astype(dtype=np.float32)))\n",
        "\n",
        "test_dataset_ext = tf.data.Dataset.from_tensor_slices((test_data.values, test_label_ext.values.astype(dtype=np.float32)))\n",
        "test_dataset_neu = tf.data.Dataset.from_tensor_slices((test_data.values, test_label_neu.values.astype(dtype=np.float32)))\n",
        "test_dataset_agr = tf.data.Dataset.from_tensor_slices((test_data.values, test_label_agr.values.astype(dtype=np.float32)))\n",
        "test_dataset_con = tf.data.Dataset.from_tensor_slices((test_data.values, test_label_con.values.astype(dtype=np.float32)))\n",
        "test_dataset_opn = tf.data.Dataset.from_tensor_slices((test_data.values, test_label_opn.values.astype(dtype=np.float32)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1x-D1g57yI4"
      },
      "source": [
        "#Prepare Data for training, validation, and testing\n",
        "\n",
        "batch_size = 128\n",
        "num_examples = tf.data.experimental.cardinality(train_dataset_ext).numpy()\n",
        "\n",
        "ext_train_batches = train_dataset_ext.shuffle(num_examples // 4).batch(batch_size).prefetch(1)\n",
        "ext_val_batches = val_dataset_ext.batch(batch_size).prefetch(1)\n",
        "ext_test_batches = test_dataset_ext.batch(batch_size)\n",
        "\n",
        "neu_train_batches = train_dataset_neu.shuffle(num_examples // 4).batch(batch_size).prefetch(1)\n",
        "neu_val_batches = val_dataset_neu.batch(batch_size).prefetch(1)\n",
        "neu_test_batches = test_dataset_neu.batch(batch_size)\n",
        "\n",
        "agr_train_batches = train_dataset_agr.shuffle(num_examples // 4).batch(batch_size).prefetch(1)\n",
        "agr_val_batches = val_dataset_agr.batch(batch_size).prefetch(1)\n",
        "agr_test_batches = test_dataset_agr.batch(batch_size)\n",
        "\n",
        "con_train_batches = train_dataset_con.shuffle(num_examples // 4).batch(batch_size).prefetch(1)\n",
        "con_val_batches = val_dataset_con.batch(batch_size).prefetch(1)\n",
        "con_test_batches = test_dataset_con.batch(batch_size)\n",
        "\n",
        "opn_train_batches = train_dataset_opn.shuffle(num_examples // 4).batch(batch_size).prefetch(1)\n",
        "opn_val_batches = val_dataset_opn.batch(batch_size).prefetch(1)\n",
        "opn_test_batches = test_dataset_opn.batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6Jp5BGiJnR9"
      },
      "source": [
        "#Create 5 Models for every personality\n",
        "\n",
        "embedding = \"https://tfhub.dev/google/nnlm-id-dim128/2\"\n",
        "\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)\n",
        "\n",
        "model_ext = tf.keras.Sequential([\n",
        "        hub_layer,\n",
        "        tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, 1)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
        "        tf.keras.layers.Dense(24, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "model_neu = tf.keras.Sequential([\n",
        "        hub_layer,\n",
        "        tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, 1)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
        "        tf.keras.layers.Dense(24, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "model_agr = tf.keras.Sequential([\n",
        "        hub_layer,\n",
        "        tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, 1)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
        "        tf.keras.layers.Dense(24, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "model_opn = tf.keras.Sequential([\n",
        "        hub_layer,\n",
        "        tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, 1)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
        "        tf.keras.layers.Dense(24, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "model_con = tf.keras.Sequential([\n",
        "        hub_layer,\n",
        "        tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, 1)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
        "        tf.keras.layers.Dense(24, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKWgRRCwGmm_"
      },
      "source": [
        "#Check the Summary of the model\n",
        "#All model have same architecture\n",
        "model_ext.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4G3sVX9Tzur"
      },
      "source": [
        "# Compile and train the EXT Model\n",
        "model_ext.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "ext_history = model_ext.fit(ext_train_batches,\n",
        "                        validation_data=ext_val_batches,\n",
        "                        epochs=6\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUwxBCQKmSea"
      },
      "source": [
        "# Compile and train the NEU Model\n",
        "model_neu.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "neu_history = model_neu.fit(neu_train_batches,\n",
        "                        validation_data=neu_val_batches,\n",
        "                        epochs=6\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9ZuTLXAmUA5"
      },
      "source": [
        "# Compile and train the AGR Model\n",
        "model_agr.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "agr_history = model_agr.fit(agr_train_batches,\n",
        "                        validation_data=agr_val_batches,\n",
        "                        epochs=6\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Li_ZUdWmUPh"
      },
      "source": [
        "# Compile and train the OPN Model\n",
        "model_opn.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "opn_history = model_opn.fit(opn_train_batches,\n",
        "                        validation_data=opn_val_batches,\n",
        "                        epochs=6\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFoadcIcmUW4"
      },
      "source": [
        "# Compile and train the CON Model\n",
        "model_con.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "con_history = model_con.fit(con_train_batches,\n",
        "                        validation_data=con_val_batches,\n",
        "                        epochs=6\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tmMljAepY9O"
      },
      "source": [
        "#Test the model\n",
        "ext_test_result = model_ext.evaluate(ext_test_batches)\n",
        "neu_test_result = model_neu.evaluate(neu_test_batches)\n",
        "agr_test_result = model_agr.evaluate(agr_test_batches)\n",
        "con_test_result = model_con.evaluate(con_test_batches)\n",
        "opn_test_result = model_opn.evaluate(opn_test_batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hao-CixcpzEm"
      },
      "source": [
        "#Code to visualize the accuracy and loss of training and validation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(ext_history.history[string])\n",
        "  plt.plot(ext_history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(ext_history, 'accuracy')\n",
        "plot_graphs(ext_history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJbsLqSYOZJO"
      },
      "source": [
        "#Save model into .h5 file so it can be uploaded into VM\n",
        "model_ext.save('ext_model.h5')\n",
        "model_neu.save('neu_model.h5')\n",
        "model_agr.save('agr_model.h5')\n",
        "model_con.save('con_model.h5')\n",
        "model_opn.save('opn_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}